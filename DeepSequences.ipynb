{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from architectures import *\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test whether aggregation moduls have correct dimensionality:\n",
    "$ f : \\mathcal{P}\\left( \\mathbb{R}^{(B \\times C \\times H \\times W)} \\right) \\rightarrow \\mathbb{R}^{(B \\times C \\times H \\times W)} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 256, 256])\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del agg\n",
    "except:\n",
    "    pass\n",
    "\n",
    "agg = AttentionAggregation().cuda()\n",
    "S,B,C,H,W = 3, 2, 64, 256, 256    # Sequence length, Batch, Channels, Height, Width\n",
    "x = torch.rand(B, S, C, H, W).cuda()\n",
    "print(agg(x).size())\n",
    "print(\"Success\")\n",
    "del agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 64, 256, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(B, S, C, H, W).permute(1,0,2,3,4).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 131072, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 64\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=64)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1).cuda()\n",
    "\n",
    "src = torch.rand(5, 2*256**2, embed_dim).cuda()\n",
    "out = transformer_encoder(src)\n",
    "del transformer_encoder\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, features, length = 256**2, 64, 10 \n",
    "multihead_attn = nn.MultiheadAttention(64,8).cuda()\n",
    "x = torch.rand(length,batch_size, features).cuda()\n",
    "attn_output, attn_output_weights = multihead_attn(x,x,x)\n",
    "\n",
    "\n",
    "(attn_output_weights > 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 2, 64, 256, 256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S,B,C,H,W = 11, 2, 64, 256, 256\n",
    "xs = [torch.zeros(B,C,H,W) for _ in range(S)]\n",
    "agg = AttentionAggregation(embed_dim=C)\n",
    "xs = agg(xs)\n",
    "xs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8.7497e-07)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(10,64,256,256)*100\n",
    "w = torch.ones(10,64,256,256)\n",
    "softm = nn.Softmax(0)\n",
    "w = softm(w)\n",
    "(x.mean(0) - (x*w).sum(0)).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
